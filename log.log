number of all smiles:  41127
Data(edge_attr=[40, 2], edge_index=[2, 40], x=[19, 2], y=0)
Data(edge_attr=[16, 2], edge_index=[2, 16], x=[9, 2], y=0)
Data(edge_attr=[58, 2], edge_index=[2, 58], x=[28, 2], y=0)
Data(edge_attr=[66, 2], edge_index=[2, 66], x=[29, 2], y=0)
Data(edge_attr=[44, 2], edge_index=[2, 44], x=[20, 2], y=0)
Data(edge_attr=[40, 2], edge_index=[2, 40], x=[19, 2], y=0)
Data(edge_attr=[36, 2], edge_index=[2, 36], x=[17, 2], y=0)
Data(edge_attr=[56, 2], edge_index=[2, 56], x=[24, 2], y=0)
Data(edge_attr=[30, 2], edge_index=[2, 30], x=[14, 2], y=0)
Data(edge_attr=[102, 2], edge_index=[2, 102], x=[45, 2], y=0)
Data(edge_attr=[42, 2], edge_index=[2, 42], x=[19, 2], y=0)
Data(edge_attr=[36, 2], edge_index=[2, 36], x=[17, 2], y=0)
Data(edge_attr=[70, 2], edge_index=[2, 70], x=[32, 2], y=0)
Data(edge_attr=[30, 2], edge_index=[2, 30], x=[15, 2], y=0)
Data(edge_attr=[54, 2], edge_index=[2, 54], x=[23, 2], y=0)
Data(edge_attr=[74, 2], edge_index=[2, 74], x=[34, 2], y=0)
Data(edge_attr=[70, 2], edge_index=[2, 70], x=[31, 2], y=0)
Data(edge_attr=[48, 2], edge_index=[2, 48], x=[23, 2], y=0)
Data(edge_attr=[32, 2], edge_index=[2, 32], x=[16, 2], y=0)
Data(edge_attr=[48, 2], edge_index=[2, 48], x=[25, 2], y=0)
Data(edge_attr=[52, 2], edge_index=[2, 52], x=[26, 2], y=0)
Data(edge_attr=[46, 2], edge_index=[2, 46], x=[19, 2], y=0)
Data(edge_attr=[36, 2], edge_index=[2, 36], x=[16, 2], y=0)
Data(edge_attr=[30, 2], edge_index=[2, 30], x=[15, 2], y=0)
Data(edge_attr=[32, 2], edge_index=[2, 32], x=[16, 2], y=1)
Data(edge_attr=[46, 2], edge_index=[2, 46], x=[20, 2], y=0)
Data(edge_attr=[48, 2], edge_index=[2, 48], x=[23, 2], y=0)
Data(edge_attr=[34, 2], edge_index=[2, 34], x=[16, 2], y=0)
Data(edge_attr=[52, 2], edge_index=[2, 52], x=[24, 2], y=0)
Data(edge_attr=[40, 2], edge_index=[2, 40], x=[19, 2], y=0)
Data(edge_attr=[26, 2], edge_index=[2, 26], x=[14, 2], y=0)
Data(edge_attr=[36, 2], edge_index=[2, 36], x=[17, 2], y=0)
Data(edge_attr=[60, 2], edge_index=[2, 60], x=[27, 2], y=0)
Data(edge_attr=[48, 2], edge_index=[2, 48], x=[22, 2], y=0)
Data(edge_attr=[18, 2], edge_index=[2, 18], x=[9, 2], y=0)
Data(edge_attr=[48, 2], edge_index=[2, 48], x=[21, 2], y=0)
[16:52:25] WARNING: not removing hydrogen atom without neighbors
[16:52:25] WARNING: not removing hydrogen atom without neighbors
Data(edge_attr=[64, 2], edge_index=[2, 64], x=[29, 2], y=0)
Data(edge_attr=[76, 2], edge_index=[2, 76], x=[35, 2], y=0)
Data(edge_attr=[70, 2], edge_index=[2, 70], x=[33, 2], y=0)
Data(edge_attr=[62, 2], edge_index=[2, 62], x=[26, 2], y=0)
Data(edge_attr=[80, 2], edge_index=[2, 80], x=[37, 2], y=0)
Data(edge_attr=[260, 2], edge_index=[2, 260], x=[117, 2], y=0)
Data(edge_attr=[32, 2], edge_index=[2, 32], x=[16, 2], y=0)
Number of training graphs: 25000
Number of test graphs: 16127
0.5479319627226904
0.5564844448917515
Epoch: 001, Loss: 0.6995, Train: 256.4019, Test: 166.2381
0.550064888886852
0.5573906893004116
Epoch: 002, Loss: 0.6535, Train: 257.5773, Test: 166.2790
0.5498230355166399
0.5571213544462337
Epoch: 003, Loss: 0.6541, Train: 256.8355, Test: 166.5196
0.5495016464743451
0.5567556808015746
Epoch: 004, Loss: 0.6553, Train: 256.7291, Test: 166.1674
0.549220240100339
0.5563864286992306
Epoch: 005, Loss: 0.6548, Train: 255.6130, Test: 166.1834
0.5488937189543531
0.5560072240114511
Epoch: 006, Loss: 0.6569, Train: 256.4101, Test: 166.1338
0.5486078011031491
0.5556777822508498
Epoch: 007, Loss: 0.6555, Train: 255.3287, Test: 166.2353
0.5483229812834326
0.5553342503131151
Epoch: 008, Loss: 0.6590, Train: 257.1152, Test: 166.1466
0.5357108483983254
0.5427542382358204
Epoch: 009, Loss: 0.6560, Train: 257.1396, Test: 166.0890
0.5357015151306829
0.5426700326534264
Epoch: 010, Loss: 0.6570, Train: 255.8690, Test: 166.0965
0.5356708179895369
0.542658402665951
Epoch: 011, Loss: 0.6567, Train: 257.0542, Test: 166.0678
0.5356343919884567
0.542550210234389
Epoch: 012, Loss: 0.6549, Train: 257.4276, Test: 166.1106
0.5006327287094559
0.5038265454464126
Epoch: 013, Loss: 0.6574, Train: 256.1620, Test: 166.0696
0.5006166879016408
0.5038187175702272
Epoch: 014, Loss: 0.6547, Train: 256.3071, Test: 166.0955
0.5006084287952359
0.5038115606548578
Epoch: 015, Loss: 0.6568, Train: 255.7397, Test: 166.0687
0.5005621682512693
0.5037761115584184
Epoch: 016, Loss: 0.6548, Train: 256.3881, Test: 166.1370
0.5005977349233592
0.5038047392198962
Epoch: 017, Loss: 0.6581, Train: 256.0226, Test: 166.0571
0.5000416774560195
0.5022287640901771
Epoch: 018, Loss: 0.6543, Train: 256.1653, Test: 166.0597
0.5000009070694895
0.5021958870101986
Epoch: 019, Loss: 0.6563, Train: 256.1458, Test: 166.0991
0.4999617598599408
0.502165246466273
Epoch: 020, Loss: 0.6547, Train: 256.6312, Test: 166.1927
0.49998161990771184
0.5013330873143675
Epoch: 021, Loss: 0.6537, Train: 255.1409, Test: 166.2220
0.49998161990771184
0.5013330873143674
Epoch: 022, Loss: 0.6542, Train: 256.5407, Test: 166.0611
0.49998161990771184
0.5013330873143675
Epoch: 023, Loss: 0.6548, Train: 256.1161, Test: 166.1312
0.49998161990771184
0.5013327518339595
Epoch: 024, Loss: 0.6545, Train: 256.1945, Test: 166.2086
0.49998161990771184
0.5013327518339595
Epoch: 025, Loss: 0.6533, Train: 255.9521, Test: 166.1257
0.4999815721672124
0.5013327518339595
Epoch: 026, Loss: 0.6594, Train: 255.9621, Test: 166.0932
0.49998161990771184
0.5013327518339595
Epoch: 027, Loss: 0.6591, Train: 256.4961, Test: 166.0893
0.49998161990771184
0.5013326400071569
Epoch: 028, Loss: 0.6568, Train: 256.1669, Test: 166.0610
0.49998152442671295
0.5013326400071569
Epoch: 029, Loss: 0.6559, Train: 254.6609, Test: 166.0585
0.49998152442671295
0.5013326400071569
Epoch: 030, Loss: 0.6567, Train: 256.2189, Test: 166.0826
0.49998152442671295
0.5013325281803542
Epoch: 031, Loss: 0.6515, Train: 256.6319, Test: 166.2245
0.49998152442671295
0.5013325281803542
Epoch: 032, Loss: 0.6540, Train: 256.3423, Test: 166.1998
0.49998152442671295
0.5013325840937556
Epoch: 033, Loss: 0.6562, Train: 255.8717, Test: 166.1885
0.4999815482969627
0.5013325281803542
Epoch: 034, Loss: 0.6590, Train: 257.5049, Test: 166.1421
0.49998152442671295
0.5013325840937556
Epoch: 035, Loss: 0.6567, Train: 257.2726, Test: 166.0566
0.49998152442671295
0.5013325840937556
Epoch: 036, Loss: 0.6558, Train: 256.5406, Test: 166.1097
0.49998152442671295
0.501332472266953
Epoch: 037, Loss: 0.6530, Train: 257.1686, Test: 166.0622
0.49998152442671295
0.5013324163535516
Epoch: 038, Loss: 0.6545, Train: 256.6243, Test: 166.0971
0.49998152442671295
0.5013324163535516
Epoch: 039, Loss: 0.6544, Train: 256.2136, Test: 166.1700
0.49998152442671295
0.5013324163535516
Epoch: 040, Loss: 0.6556, Train: 256.0877, Test: 166.0604
0.49998152442671295
0.5013324163535516
Epoch: 041, Loss: 0.6560, Train: 256.0818, Test: 166.0594
0.49998152442671295
0.501332472266953
Epoch: 042, Loss: 0.6539, Train: 256.6133, Test: 166.1140
0.49998152442671295
0.5013323604401503
Epoch: 043, Loss: 0.6529, Train: 256.0234, Test: 166.1058
0.49998152442671295
0.5013324163535516
Epoch: 044, Loss: 0.6582, Train: 255.9082, Test: 166.0618
0.49998152442671295
0.5013323045267489
Epoch: 045, Loss: 0.6544, Train: 256.3605, Test: 166.1034
0.49998152442671295
0.5013323604401502
Epoch: 046, Loss: 0.6573, Train: 256.1136, Test: 166.0872
0.49998152442671295
0.5013323604401503
Epoch: 047, Loss: 0.6557, Train: 256.3321, Test: 166.0734
0.49998152442671295
0.5013323045267489
Epoch: 048, Loss: 0.6564, Train: 255.8694, Test: 166.1094
0.49998152442671295
0.501332136786545
Epoch: 049, Loss: 0.6556, Train: 256.3538, Test: 166.0613
0.49998152442671295
0.5013323045267489
Epoch: 050, Loss: 0.6555, Train: 256.9458, Test: 166.0565
0.49998152442671295
0.501331969046341
Epoch: 051, Loss: 0.6567, Train: 255.7525, Test: 166.0670
0.4999815482969627
0.5013320249597424
Epoch: 052, Loss: 0.6567, Train: 256.5381, Test: 166.1621
0.4999815721672124
0.5013316335659331
Epoch: 053, Loss: 0.6563, Train: 257.1026, Test: 166.1716
0.4999814766862135
0.5013319131329397
Epoch: 054, Loss: 0.6557, Train: 256.3676, Test: 166.1594
0.4999818347399594
0.5013335346215781
Epoch: 055, Loss: 0.6567, Train: 257.0741, Test: 166.0772
0.4999712363490816
0.5013013844158168
Epoch: 056, Loss: 0.6564, Train: 256.6692, Test: 166.0587
0.499970329279592
0.5012979736983361
Epoch: 057, Loss: 0.6555, Train: 256.6072, Test: 166.2055
0.4999702099283434
0.5012977500447308
Epoch: 058, Loss: 0.6568, Train: 256.5080, Test: 166.0596
0.49997092603583515
0.5012973586509215
Epoch: 059, Loss: 0.6559, Train: 255.9421, Test: 166.1530
0.4999704963713401
0.5012981973519414
Epoch: 060, Loss: 0.6553, Train: 256.4681, Test: 166.0584
0.49997168988382634
0.5012974704777241
Epoch: 061, Loss: 0.6554, Train: 256.6744, Test: 166.1237
0.49997262082356564
0.5012743782429773
Epoch: 062, Loss: 0.6570, Train: 257.4961, Test: 166.0614
0.49997004283659535
0.5012759997316156
Epoch: 063, Loss: 0.6581, Train: 256.1749, Test: 166.0602
0.5013733748179177
0.5015377862766148
Epoch: 064, Loss: 0.6571, Train: 256.2376, Test: 166.2158
0.5013734225584172
0.5015358852209698
Epoch: 065, Loss: 0.6553, Train: 256.4556, Test: 166.0593
0.5013733270774182
0.5015344873859366
Epoch: 066, Loss: 0.6538, Train: 256.1993, Test: 166.0865
0.4994609142802145
0.5001615338164251
Epoch: 067, Loss: 0.6590, Train: 255.6418, Test: 166.0824
0.4994610813719626
0.5001608628556093
Epoch: 068, Loss: 0.6558, Train: 255.2074, Test: 166.0643
0.4994608904099648
0.500158905886563
Epoch: 069, Loss: 0.6577, Train: 255.5464, Test: 166.0564
0.4994610575017129
0.5001590177133656
Epoch: 070, Loss: 0.6567, Train: 256.5394, Test: 166.0580
0.4994609381504643
0.5001624843442477
Epoch: 071, Loss: 0.6561, Train: 256.3912, Test: 166.1202
0.49946086653971505
0.5001582349257471
Epoch: 072, Loss: 0.6566, Train: 256.7349, Test: 166.0643
0.4994608904099648
0.5001550478618716
Epoch: 073, Loss: 0.6537, Train: 256.1953, Test: 166.0855
